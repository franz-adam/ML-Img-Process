
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
#################################################
# file to edit: solution.ipynb͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

import numpy as np
from helper_functions import *

def get_initial_means(array, k):
    """
    Picks k random points from the 2D array
    (without replacement) to use as initial
    cluster means

    params:
    array = numpy.ndarray[numpy.ndarray[float]] - m x n | datapoints x features

    k = int

    returns:
    initial_means = numpy.ndarray[numpy.ndarray[float]]
    """
    k = min(k, array.shape[0])
    # Randomly sample rows from array
    indices = np.random.choice(array.shape[0], size=k, replace=False)
    initial_k_means = array[indices]

    return initial_k_means

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def k_means_step(X, k, means):
    """
    A single update/step of the K-means algorithm
    Based on a input X and current mean estimate,
    predict clusters for each of the pixels and
    calculate new means.
    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n | pixels x features (already flattened)
    k = int
    means = numpy.ndarray[numpy.ndarray[float]] - k x n

    returns:
    (new_means, clusters)
    new_means = numpy.ndarray[numpy.ndarray[float]] - k x n
    clusters = numpy.ndarray[int] - m sized vector
    """
    distances = np.linalg.norm(X[:, np.newaxis] - means, axis=2)  # shape (m, k)

    # Assign clusters based on closest mean
    clusters = np.argmin(distances, axis=1)  # shape (m,)

    # Update means
    new_means = np.array([X[clusters == j].mean(axis=0) if np.any(clusters == j) else means[j] for j in range(k)])

    return new_means, clusters

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def k_means_segment(image_values, k=3, initial_means=None):
    """
    Separate the provided RGB values into
    k separate clusters using the k-means algorithm,
    then return an updated version of the image
    with the original values replaced with
    the corresponding cluster values.

    params:
    image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch
    k = int
    initial_means = numpy.ndarray[numpy.ndarray[float]] or None

    returns:
    updated_image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch
    """
    max_iterations = 150
    threshold = 1e-7

    # Get original image shape
    rows, columns, color_channels = image_values.shape

    # Flatten image_values from rows x columns x RGB to 2d array
    img_flat = image_values.reshape(-1, color_channels)

    # If initial_means is None, set assign them randomly
    if initial_means is None:
        initial_means = get_initial_means(img_flat, k)

    old_means = initial_means.copy()

    for _ in range(max_iterations):
        new_means, clusters = k_means_step(X=img_flat, k=k, means=old_means)

        # Check for convergence
        mean_shift = np.max(np.linalg.norm(new_means - old_means, axis=1))
        if mean_shift < threshold:
            break

        old_means = new_means.copy()

    # Create a new array for the segmented image
    segmented_img_flat = new_means[clusters]

    img_processed = segmented_img_flat.reshape(rows, columns, color_channels)

    return img_processed

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

"""
Make sure to put #export (first line in this cell) only
if you call/use this function elsewhere in the code
"""
def compute_sigma(X, MU):
    """
    Calculate covariance matrix, based in given X and MU values

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n

    returns:
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    """
    # Initialize variables
    m, n = X.shape
    k = MU.shape[0]
    SIGMA = np.zeros((k, n, n))

    # Calculate the covariance matrix for each cluster
    for i in range(k):
        # Find the data points assigned to the current cluster
        cluster_points = X - MU[i]

        # Compute the covariance matrix for the current cluster
        SIGMA[i] = np.dot(cluster_points.T, cluster_points) / cluster_points.shape[0]

    return SIGMA

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def initialize_parameters(X, k):
    """
    Return initial values for training of the GMM
    Set component mean to a random
    pixel's value (without replacement),
    based on the mean calculate covariance matrices,
    and set each component mixing coefficient (PIs)
    to a uniform values
    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int

    returns:
    (MU, SIGMA, PI)
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    """
    m, n = X.shape
    indices = np.random.choice(m, k, replace=False)
    MU = X[indices]

    SIGMA = compute_sigma(X, MU)

    # Initialize the mixing coefficients to uniform values
    PI = np.full(k, 1/k)

    return MU, SIGMA, PI

    # TODO: finish this function͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
    # Hint: for initializing SIGMA you could choose to use compute_sigma͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
    raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def prob(x, mu, sigma):
    """Calculate the probability of x (a single
    data point or an array of data points) under the
    component with the given mean and covariance.
    The function is intended to compute multivariate
    normal distribution, which is given by N(x;MU,SIGMA).

    params:
    x = numpy.ndarray[float] (for single datapoint)
        or numpy.ndarray[numpy.ndarray[float]] (for array of datapoints)
    mu = numpy.ndarray[float]
    sigma = numpy.ndarray[numpy.ndarray[float]]

    returns:
    probability = float (for single datapoint)
                or numpy.ndarray[float] (for array of datapoints)
    """
    single_dist = mu.ndim == 1
    if single_dist:
        mu = mu[np.newaxis, :]
        sigma = sigma[np.newaxis, :, :]

    # Ensure x is 2D
    x = np.atleast_2d(x)

    # Compute the normalization factor
    n = mu.shape[1]
    sigma_det = np.linalg.det(sigma)
    sigma_inv = np.linalg.inv(sigma)
    normalization_factor = np.sqrt((2 * np.pi) ** n * sigma_det)

    # Calculate the difference and the exponent
    diff = x[:, np.newaxis, :] - mu
    exponent = -0.5 * np.sum(np.einsum('mkn,knj->mkj', diff, sigma_inv) * diff, axis=2)

    # Compute the probability densities
    density = np.exp(exponent) / normalization_factor

    # Return the correct shape based on the input
    if single_dist:
        return density[:, 0] if x.shape[0] > 1 else density[0, 0]
    else:
        return density

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def E_step(X,MU,SIGMA,PI,k):
    """
    E-step - Expectation
    Calculate responsibility for each
    of the data points, for the given
    MU, SIGMA and PI.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    k = int

    returns:
    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m
    """
    densities = prob(X, MU, SIGMA)  # shape (m, k)

    # Weight the densities by the mixing coefficients
    weighted_densities = densities * PI  # Broadcasting PI across each column

    # Sum across components for normalization and reshape for broadcasting
    sum_densities = np.sum(weighted_densities, axis=1, keepdims=True)

    # Compute the responsibilities
    responsibility = (weighted_densities / sum_densities).T  # shape (k, m)

    return responsibility

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def M_step(X, r, k):
    """
    M-step - Maximization
    Calculate new MU, SIGMA and PI matrices
    based on the given responsibilities.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    r = numpy.ndarray[numpy.ndarray[float]] - k x m
    k = int

    returns:
    (new_MU, new_SIGMA, new_PI)
    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    new_PI = numpy.ndarray[float] - k
    """
    m, n = X.shape
    r_transposed = r.T  # shape (m, k)

    # Calculate the new means
    weight_sums = np.sum(r_transposed, axis=0)
    new_MU = np.dot(r_transposed.T, X) / weight_sums[:, np.newaxis]  # shape (k, n)

    # Calculate the new covariance matrices
    new_SIGMA = np.zeros((k, n, n))
    for j in range(k):
        X_centered = X - new_MU[j]  # shape (m, n)
        weighted_X_centered = r_transposed[:, j][:, np.newaxis] * X_centered  # shape (m, n)
        new_SIGMA[j] = np.dot(weighted_X_centered.T, X_centered) / weight_sums[j]  # shape (n, n)

    # Update the mixing coefficients
    new_PI = weight_sums / m  # shape (k,)

    return new_MU, new_SIGMA, new_PI

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def likelihood(X, PI, MU, SIGMA, k):
    """Calculate a log likelihood of the
    trained model based on the following
    formula for posterior probability:

    log(Pr(X | mixing, mean, stdev)) = sum((i=1 to m), log(sum((j=1 to k),
                                      mixing_j * N(x_i | mean_j,stdev_j))))

    Make sure you are using natural log, instead of log base 2 or base 10.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    k = int

    returns:
    log_likelihood = float
    """
     # Compute the probability densities for all points and clusters
    densities = prob(X, MU, SIGMA)  # shape (m, k)

    # Compute the weighted sum of densities across all clusters
    weighted_densities = densities * PI  # shape (m, k)
    mixture_prob = np.sum(weighted_densities, axis=1)  # shape (m,)

    # Compute the log likelihood
    total_log_likelihood = np.sum(np.log(mixture_prob))

    return total_log_likelihood

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def train_model(X, k, convergence_function, initial_values = None):
    """
    Train the mixture model using the
    expectation-maximization algorithm.
    E.g., iterate E and M steps from
    above until convergence.
    If the initial_values are None, initialize them.
    Else it's a tuple of the format (MU, SIGMA, PI).
    Convergence is reached when convergence_function
    returns terminate as True,
    see default convergence_function example
    in `helper_functions.py`

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int
    convergence_function = func
    initial_values = None or (MU, SIGMA, PI)

    returns:
    (new_MU, new_SIGMA, new_PI, responsibility)
    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    new_PI = numpy.ndarray[float] - k
    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m
    """
    n = X.shape[1]
    if initial_values is None:
        MU = np.random.rand(k, n)
        SIGMA = np.array([np.eye(n) for _ in range(k)])
        PI = np.full(k, 1/k)
    else:
        MU, SIGMA, PI = initial_values

    conv_ctr = 0
    prev_likelihood = None  # Initialize to None to check for the first iteration

    max_iterations = 700

    for iteration in range(max_iterations):
        # E-step
        responsibility = E_step(X, MU, SIGMA, PI, k)

        # M-step
        MU, SIGMA, PI = M_step(X, responsibility, k)

        # Log likelihood for convergence check
        new_likelihood = likelihood(X, PI, MU, SIGMA, k)

        if prev_likelihood is not None:
            # Check for convergence
            conv_ctr, converged = convergence_function(prev_likelihood, new_likelihood, conv_ctr)
            if converged:
                #print(f"Converged after {iteration} iterations.")
                break

        #print(f"Iteration {iteration}, Log Likelihood: {new_likelihood}")

        prev_likelihood = new_likelihood  # Update previous likelihood for next iteration check

    return MU, SIGMA, PI, responsibility

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def cluster(r):
    """
    Based on a given responsibilities matrix
    return an array of cluster indices.
    Assign each datapoint to a cluster based,
    on component with a max-likelihood
    (maximum responsibility value).

    params:
    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix

    return:
    clusters = numpy.ndarray[int] - m x 1
    """
    clusters = np.argmax(r, axis=0)

    return clusters

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def segment(X, MU, k, r):
    """
    Segment the X matrix into k components.
    Returns a matrix where each data point is
    replaced with its max-likelihood component mean.
    E.g., return the original matrix where each pixel's
    intensity replaced with its max-likelihood
    component mean. (the shape is still mxn, not
    original image size)

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    k = int
    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix

    returns:
    new_X = numpy.ndarray[numpy.ndarray[float]] - m x n
    """
    cluster_indices = np.argmax(r, axis=0)
    new_X = MU[cluster_indices]

    return new_X

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def best_segment(X,k,iters):
    """Determine the best segmentation
    of the image by repeatedly
    training the model and
    calculating its likelihood.
    Return the segment with the
    highest likelihood.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int
    iters = int

    returns:
    (likelihood, segment)
    likelihood = float
    segment = numpy.ndarray[numpy.ndarray[float]]
    """
    best_likelihood = -np.inf
    best_segment = None

    for _ in range(iters):
        # Train the model
        MU, SIGMA, PI, r = train_model(X, k, default_convergence)

        # Calculate the likelihood of the current model
        current_likelihood = likelihood(X, PI, MU, SIGMA, k)

        # Check if the current model is better than the best one found so far
        if current_likelihood > best_likelihood:
            best_likelihood = current_likelihood
            # Create the segmentation for the current model
            best_segment = segment(X, MU, k, r)

    return best_likelihood, best_segment

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def improved_initialization(X,k):
    """
    Initialize the training
    process by setting each
    component mean using some algorithm that
    you think might give better means to start with,
    based on the mean calculate covariance matrices,
    and set each component mixing coefficient (PIs)
    to a uniform values
    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int

    returns:
    (MU, SIGMA, PI)
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    """
    m, n = X.shape

    # Step 1: Randomly select an initial point as the first mean
    indices = np.random.choice(m, size=1)
    MU = X[indices, :]

    # Step 2: Iteratively select remaining means by maximizing the minimum distance to existing means
    for _ in range(1, k):
        distances = np.min(np.linalg.norm(X[:, np.newaxis] - MU, axis=2), axis=1)
        new_index = np.argmax(distances)
        MU = np.vstack([MU, X[new_index:new_index+1, :]])

    # Calculate initial covariances (SIGMA) and mixing coefficients (PI) uniformly
    SIGMA = np.array([np.cov(X.T) for _ in range(k)])  # Start with global covariance as approximation
    PI = np.full(k, 1/k)

    return MU, SIGMA, PI

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def new_convergence_function(previous_variables, new_variables, conv_ctr,
                             conv_ctr_cap=10):
    """
    Convergence function
    based on parameters:
    when all variables vary by
    less than 10% from the previous
    iteration's variables, increase
    the convergence counter.

    params:
    previous_variables = [numpy.ndarray[float]]
                         containing [means, variances, mixing_coefficients]
    new_variables = [numpy.ndarray[float]]
                    containing [means, variances, mixing_coefficients]
    conv_ctr = int
    conv_ctr_cap = int

    return:
    (conv_crt, converged)
    conv_ctr = int
    converged = boolean
    """
    prev_means, prev_variances, prev_mixing_coeffs = previous_variables
    new_means, new_variances, new_mixing_coeffs = new_variables

    # Calculate the percentage change for means, variances, and mixing coefficients
    means_change = np.linalg.norm(new_means - prev_means) / np.linalg.norm(prev_means)
    variances_change = np.linalg.norm(new_variances - prev_variances) / np.linalg.norm(prev_variances)
    mixing_coeffs_change = np.linalg.norm(new_mixing_coeffs - prev_mixing_coeffs) / np.linalg.norm(prev_mixing_coeffs)

    # Check if all changes are within 10%
    if means_change < 0.1 and variances_change < 0.1 and mixing_coeffs_change < 0.1:
        conv_ctr += 1
    else:
        conv_ctr = 0

    # Check if the convergence counter has reached the cap
    converged = conv_ctr >= conv_ctr_cap

    return conv_ctr, converged

def train_model_improved(X, k, convergence_function, initial_values = None):
    """
    Train the mixture model using the
    expectation-maximization algorithm.
    E.g., iterate E and M steps from
    above until convergence.
    If the initial_values are None, initialize them.
    Else it's a tuple of the format (MU, SIGMA, PI).
    Convergence is reached when convergence_function
    returns terminate as True. Use new_convergence_fuction
    implemented above.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int
    convergence_function = func
    initial_values = None or (MU, SIGMA, PI)

    returns:
    (new_MU, new_SIGMA, new_PI, responsibility)
    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    new_PI = numpy.ndarray[float] - k
    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m
    """
    n = X.shape[1]
    if initial_values is None:
        MU = np.random.rand(k, n)
        SIGMA = np.array([np.eye(n) for _ in range(k)])
        PI = np.full(k, 1/k)
    else:
        MU, SIGMA, PI = initial_values

    conv_ctr = 0
    previous_variables = [MU, SIGMA, PI]

    for iteration in range(700):  # You can set a max_iterations variable if needed
        # E-step
        responsibility = E_step(X, MU, SIGMA, PI, k)

        # M-step
        new_MU, new_SIGMA, new_PI = M_step(X, responsibility, k)
        new_variables = [new_MU, new_SIGMA, new_PI]

        # Check for convergence using the new convergence function
        conv_ctr, converged = convergence_function(previous_variables, new_variables, conv_ctr)

        if converged:
            break

        # Update variables for the next iteration
        MU, SIGMA, PI = new_MU, new_SIGMA, new_PI
        previous_variables = [MU, SIGMA, PI]

    return MU, SIGMA, PI, responsibility

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
# Unittest below will check both of the functions at the same time.͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def bayes_info_criterion(X, PI, MU, SIGMA, k):
    """
    See description above
    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    k = int

    return:
    bayes_info_criterion = int
    """
    m, n = X.shape

    # Calculate the total number of parameters in the model
    num_params = k * n  # means
    num_params += k * (n * (n + 1) / 2)  # covariance matrices
    num_params += (k - 1)  # mixing coefficients

    # Calculate the maximized log-likelihood
    # For simplicity, assuming we can call a function `log_likelihood` that computes it
    log_likelihood = likelihood(X, PI, MU, SIGMA, k)

    # Compute the BIC
    bic = np.log(m) * num_params - 2 * log_likelihood

    return bic

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
################ END OF LOCAL TEST CODE SECTION ######################͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁

def BIC_likelihood_model_test(image_matrix, comp_means):
    """Returns the number of components
    corresponding to the minimum BIC
    and maximum likelihood with respect
    to image_matrix and comp_means.

    params:
    image_matrix = numpy.ndarray[numpy.ndarray[float]] - m x n
    comp_means = list(numpy.ndarray[numpy.ndarray[float]]) - list(k x n) (means for each value of k)

    returns:
    (n_comp_min_bic, n_comp_max_likelihood)
    n_comp_min_bic = int
    n_comp_max_likelihood = int
    """
    min_bic = np.inf
    max_likelihood = -np.inf
    n_comp_min_bic = 0
    n_comp_max_likelihood = 0

    for means in comp_means:
        k = means.shape[0]  # Number of clusters/components for this model

        # Ensure means is two-dimensional
        means = np.atleast_2d(means)

        # Initialize SIGMA and PI if necessary
        n = image_matrix.shape[1]
        initial_SIGMA = np.array([np.eye(n) for _ in range(k)])
        initial_PI = np.full(k, 1/k)

        initial_values = (means, initial_SIGMA, initial_PI)

        # Train the model
        try:
            MU, SIGMA, PI, _ = train_model_improved(image_matrix, k, new_convergence_function, initial_values=initial_values)
        except Exception as e:
            print(f"EXCEPTION IN train_model_improved: {e}")
            continue

        # Calculate BIC and likelihood for the trained model
        current_bic = bayes_info_criterion(image_matrix, PI, MU, SIGMA, k)
        current_likelihood = likelihood(image_matrix, PI, MU, SIGMA, k)

        # Update the min BIC and max likelihood information
        if current_bic < min_bic:
            min_bic = current_bic
            n_comp_min_bic = k

        if current_likelihood > max_likelihood:
            max_likelihood = current_likelihood
            n_comp_max_likelihood = k

    return n_comp_min_bic, n_comp_max_likelihood


def return_your_name():
    # return your name͏󠄂͏️͏󠄌͏󠄎͏󠄎͏󠄊͏󠄁
    return "Franz Adam"